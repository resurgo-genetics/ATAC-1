---
title: "A5-all-t-treg"
author: "JRD"
date: "February 12, 2016"
output: html_document
---

last modified: `r Sys.Date()`
<br>    

#### README
+ this document contains code and explanation for an analysis comparing Colon, Muscle, Splenic and VAT Treg
+ for details on how the input files used in this analysis were generated, see *ATAC_mapping.Rmd* and *ATAC_peaks.Rmd*
<br>    


#### R Markdown Specifications
Set global options
```{r setoptions, echo=F}
library(knitr)
opts_chunk$set(echo=F, eval=F, warning=F, message=F)
```
<br>    

Github
https://github.com/jdis24/ATAC.git
<br>    

#### Pseudo
**call peaks and get read counts**
+ [these steps were done in "ATAC_peaks.Rmd"]
+ for each sample, call peaks in each replicate
+ then merge into single peakset
+ for each replicate, count reads in each peak, normalized to total library size
+ export .csv files of peaks and counts
**filter for strong peaks, merge peaks and get read counts for merged peaks**
+ [all subsequent steps done in this document]
+ plot distributions of read counts 
+ filter for strong peaks (start with >=2.5 rpm)
+ get genomic annotations of strong peaks (compare to unfilterd to check degree of TSS enrichment)
+ use DiffBind to create merged peakset across all T-treg and get read counts
+ export all peaks and their rpkm score in each sample 
**start clustering analysis**
+ test for outliers here? 
+ convert rpkm values to log2 and take average across replicates to get single value for each cell type
+ using matrix of log2 rpkm values: 
+ test for any outliers (very high or low rpkm in all samples?)
+ start with hclust on random sample of peaks - sample size? 
++ can I use hc results as initial centroids for Kmeans?
<br>    

#### Import peak files into peak list and export bed files
```{r peak_list, eval=T}
# create path to .csv files  
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Master_peaks/R/peaks_with_counts/working/"

# get filenames
filenames1 <- list.files(path = path, pattern = ".csv")
filenames <- paste(path, filenames1, sep="") # update filenames with path

# combine peak table .csv dfs into a list
colClass <- c(rep("factor",2), rep("numeric",3), "character", rep("numeric",4)) # set column classes
pl <- lapply(filenames, read.csv, colClasses = colClass) # [p]eak [l]ist

# name the list elements to match the files
names(pl) <- gsub("\\.csv", "", filenames1)

# convert each df to bed format
pl_bed <- lapply(pl, function(x) {
  x <- x[,c("seqnames","start","end")]
})

# export each df as tdt bed file
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/bed/"

lapply(names(pl_bed),
       function(x) write.table(pl_bed[x], file=paste(paste0(path,x),"bed",sep="."), sep = "\t", 
                              quote=F, row.names=F,col.names=c("#chr","start","end")))
```
<br>    


#### Generate table of peak numbers for range of rpm threshold values
```{r threshold_table, eval=T, results="asis"}
library(xtable)

# function to compute the number of peaks >=n rpm in each sample
# ARGS: df = dataframe of peak height rpms, peak_height_var = string of var name storing rpm values, cutoffs = numeric vector of rpm cutoffs 
# USAGE: number_peaks_above_nrpm(df, "peak_height_var", cutoffs)
number_peaks_above_nrpm <- function(df, rpm_var, cutoffs) { 
  # create indexing vector representing number of elements in cutoffs
  n <- length(cutoffs) 
  # create output vector whose length = length of cutoffs
  o <- numeric(n) 
  # create variable to store vector of extracted rpm values
  rpm <- df[[rpm_var]]
  # loop over each element of cutoffs and count number rows where peak height >= that element
  for(i in 1:n) { 
    o[i] <- as.numeric(sum(rpm >= cutoffs[[i]])) # calculate the total number of rows where rpm >= ith value in cutoffs  
  }
  # print output vector
  return(o) 
}


# create cutoff vector of rpm values
cutoffs <- c(seq(0,5,by=0.5))

# apply cutoffs to list
peaks_pass_filter <- sapply(pl, number_peaks_above_nrpm, "rpm", cutoffs)

# transform list into dataframe (if doesn't work may have to call base as.data.frame bc of Biocgenerics masking?)
ppf <- as.data.frame(peaks_pass_filter) #[p]eaks [p]ass [f]ilter
ppf <- cbind(ppf,"rpm_cutoff" = cutoffs) # add column for each cutoff

# write to file
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/"
write.csv(ppf, file = paste0(path,"peak_totals_rpm_filtered.csv"), row.names=F)

# generate xtable
print(xtable(ppf), type="html", include.rownames=F)

# plot distributions of total peaks called at each cutoff
library(reshape2)
library(ggplot2)

ppfm <- melt(ppf, id.vars='rpm_cutoff',variable.name='no_peaks') # ppf[m]elted 
g <- ggplot(ppfm, aes(x=as.factor(rpm_cutoff),y=value))
g + geom_boxplot() +
  theme_bw()
ggsave(paste0(path, "a5-peakcut-boxplot.pdf"), width=7, height=5) # export pdf
```
<br>    


#### Plot distribution of read counts for each sample and then filter by rpm 
```{r plots, eval=T}
library(reshape2)
library(ggplot2)
library(dplyr)
# set path for plots 
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/"

# get peak list into shape for plotting
plm <- melt(pl, id.vars=names(pl[[1]])) # [p]eak [l]ist [m]elted
plm$L1 <- NULL # remove extraneous column

# calculcate log2 rpm values and append as new column
plm$log2_rpm <- log(plm$rpm, 2)

# show number of peaks where rpm =0 
rpm0pks <- plm$log2_rpm == "-Inf"
plm_rpm0pks <- plm[rpm0pks,]

print("Peaks where rpm =0 ...") 
xtable(plm_rpm0pks)

# filter out peaks where rpm = 0
plmf <- plm[!rpm0pks,]  #plm[f]iltered

# draw boxplot of all peaks in each sample
g <- ggplot(data = plmf, aes(x=Sample, y=log2_rpm, fill=Sample))
g + geom_boxplot(varwidth =T) +
  theme_bw() + 
  ylim(-2,10) +
  theme(axis.title.x = element_blank())
ggsave(paste0(path, "a5-boxplot.pdf"), width=7, height=5) # export pdf

# draw boxplot of peaks passing n rpm
# define function to filter peaks and draw plot
bp_nrpm <- function(df, n) {
  df %>%
    filter(rpm >= n) %>% # filter by rpm cutoff 
    ggplot(aes(x=Sample, y=log2_rpm, fill=Sample)) + # plot
    geom_boxplot(varwidth=T) +
    ylim(-2,10) +
    theme_bw() + 
    theme(axis.title.x = element_blank())
}

n <- 2 # set rpm cutoff
bp_nrpm(plmf, n) # draw plot
ggsave(paste0(path,"a5-boxplot-",n,"rpm-filtered.pdf"), width =7, height =5) # export pdf

# draw histograms of all peaks in each sample
g <- ggplot(data = plmf, aes(x=log2_rpm))
g + geom_histogram(fill="white",color="black",binwidth=0.1) +
  facet_grid(Sample~.) +
  geom_vline(xintercept =log(1,2)) +
  geom_vline(xintercept = log(2,2)) +
  geom_vline(xintercept = log(3,2)) +
  theme_bw()
ggsave(paste0(path, "a5-count-hist.pdf"), width=5, height=25)
```
<br>    


#### Filter for strong peaks
```{r filter_peaks, eval=T}
library(dplyr)
# select peaks >= nrpm
n <- 2
sp <- lapply(pl, filter, rpm >=n) #[s]trong[p]eaks

# export each df as .csv file
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/csv/"
suffix <- paste(n,'rpm','.csv',sep="")
lapply(names(sp),
       function(x) write.csv(sp[[x]], file=paste(path,x,'-',suffix,sep=""), row.names=F))

# convert each df to bed format
spbed <- lapply(sp, function(x) {
  x <- x[,c("seqnames","start","end")]
})

# export each df as tdt bed file
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/bed/"

suffix <- paste(n, 'rpm','.bed',sep="")
lapply(names(spbed),
       function(x) write.table(spbed[x], file=paste(path,x,'-',suffix,sep=""), sep = "\t",
                               quote=F, row.names=F,col.names=c("#chr","start","end")))
```
<br>    


#### Get genomic annotations of all peaks and strong peaks
+ are they biased for TSS?
+ HOMER annotatePeaks.pl
+ TODO
```{r annotate_pks, engine="bash"}
annotatePeaks.pl <filenmame> mm9 -annStats <filenmame>.annStats.txt > <filenmame>.annpks.txt

annotatePeaks.pl <filenmame>.bed mm9 -annStats <filenmame>.annStats.txt > <filenmame>.annpks.txt
```
<br> 


#### Create merged peak set and get normalized read counts 
+ DiffBind 
+ Used R 3.2.1 on Orchestra so didn't have to keep .bam locally
+ script called `a5.db.counts.R`
+ run with:
`module load stats/R/3.2.1`
`bsub -q priority -W 5:00 -R "rusage[mem=64000]" Rscript a5.db.counts.R`
```{r merge_and_count}
library(DiffBind)

##----- create sample sheet --------------------------------------------------------------------------------
path <- "/groups/cbdm-db/jrd26/ATAC_crossbatch/R/Analysis5/"

Peaks <- list.files(paste0(path, "bed/2rpm-filtered/"))

# create columns for sample sheet
SampleID <- unlist(strsplit(Peaks, split = "-2rpm.bed"))
Tissue <- c(rep('Colon',4),rep('Muscle',2),rep('Spleen',8),rep("VAT",2))
Factor <- unlist(strsplit(SampleID, split = "_rep1.2"))
Factor <- unlist(strsplit(Factor, split = "_rep1"))
Factor <- unlist(strsplit(Factor, split = "_rep2"))
Factor <- unlist(strsplit(Factor, split = "_rep3"))
Condition <- "Treg"
Treatment <- c('Batch1.2','Batch2','Batch1.2','Batch2',rep('Batch2',2),'Batch1','Batch2',
               'Batch1','Batch2',rep('Batch2',2),rep('Batch1',4))
Replicate <- rep(c('1','2'),8)
PeakCaller <- "homer"
PeakFormat <- "bed"
PeakPath <- list.files(paste0(path, "bed/2rpm-filtered/"), full.names = T)
bamPath <- list.files(paste0(path,"reads/"), full.names=T)

samples <- cbind(SampleID = SampleID, 
                 Tissue = Tissue, 
                 Factor = Factor, 
                 Condition = Condition,
                 Treatment = Treatment,
                 Replicate = Replicate,
                 bamReads = bamPath,
                 Peaks = PeakPath,
                 PeakCaller = PeakCaller,
                 PeakFormat = PeakFormat)
write.csv(samples, "samples.csv", row.names=F)


##-----loading data-----------------------------------------------------------------------------------
# create DBA object
ttreg <- dba(sampleSheet = "samples.csv", bRemoveRandom = T, minOverlap = 2)

# make consensus peakset from replicates
ttreg <- dba.peakset(ttreg, consensus = DBA_FACTOR)

# make consensus peakset from samples and count reads
ttreg <- dba.count(ttreg, peaks = ttreg$masks$Consensus, score = DBA_SCORE_RPKM)

# save dba to file
n <- 2  # save rpm filter value
dba.save(ttreg, file=paste0("a5-ttreg",n,'rpm-filtered'), dir=path, pre="dba_", ext="RData", bMinimize=F)

# export consensus peakset with read counts
# set filter
n <- 2
suffix <- paste('a5-ttreg-consensus-',n,'rpm-filtered-rpkm',sep="")
ttreg_con <- dba.peakset(ttreg, ttreg$masks$Consensus, bRetrieve = T, 
                         writeFile=paste0(path,suffix,'.txt'), DataType=DBA_DATA_FRAME)

write.csv(ttreg_con, file=paste0(path, suffix, '.csv'), row.names=F)

# save read count correlation heatmap
suffix <- paste('a5-ttreg-',n,'rpm-reads-heat',sep="")
pdf(paste0(path,suffix,'.pdf'), width=10, height=10, pagecentre = T)
par(oma = c(3,2,2,3))
dba.plotHeatmap(ttreg)
dev.off()

# save read count PCA
suffix <- paste('a5-ttreg-',n,'rpm-pca',sep="")
pdf(paste0(path,suffix,'.pdf'), width=10, height=10, pagecentre = T)
par(oma = c(3,2,2,3))
dba.plotPCA(ttreg, attributes=DBA_TISSUE, label=DBA_FACTOR, score=DBA_SCORE_RPKM)
dev.off()

```
<br>  


#### Quantile normalize read count values
**Pseudo:**  
+ Plot distributions of read counts in the merged peak set for all samples 
+ Quantile normalize using bioconductor...
+ plot distributions of read counts in the merged peak set post QN
```{r qn}
library(reshape2)
library(ggplot2)

# import peak file
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/2rpm-filtered/csv/"

ttreg <- read.csv(file = paste(path,'a5-ttreg-consensus-2rpm-filtered-rpkm.csv',sep=""), header =T)

##----plot read count distributions after merge -------------------------------------------------------------
# set plots path
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/2rpm-filtered/plots/"

# set file suffix
suffix <- paste0('a5-ttreg-', n,'rpm-filtered-merged-counts-rpkm')

# plot and save
pdf(paste0(path,suffix,'.pdf'), width=8, height=6)
par(oma = c(4,1,0.5,1))
h <- 4 # set height of horizontal line
boxplot(log(ttreg[4:19],2), las = 2, cex.axis = 0.7, 
        ylab='log2 rpkm', 
        main=paste('merged atac peaks ','n=',nrow(ttreg),'; abline rpm=',h,sep=""))
abline(h=h, col = 'red')
dev.off()

# try rotated view
# boxplot(log(ttreg[4:19],2), las = 2, cex.axis = 0.5, horizontal =T)

##---- quantile normalization ----------------------------------------------------------------------------
library(preprocessCore)

# create matrix of rpm values
ttrm <- as.matrix(ttreg[,4:19]) # [t][t][r]eg[m]atrix

# qn using preprocess core
ttrqnmat <- normalize.quantiles(ttrm) #[t][t][r]eg[q]uantile[n]ormalized[mat]rix

# set file suffix
suffix <- paste0('a5-ttreg-', n,'rpm-filtered-merged-counts-rpkm-qn')

##----plot read count distributions after merge then qn ---------------------------------------------------

# plot and save
pdf(paste0(path,suffix,'.pdf'), width=8, height=6)
par(oma = c(4,1,0.5,1))
h <- 4 # set height of horizontal line
boxplot(log(ttrqnmat,2), las = 2, cex.axis = 0.7, 
        ylab='log2 rpkm', 
        main=paste('merged atac peaks ','n=',nrow(ttreg),'; abline rpm=',h,sep=""))
abline(h=h, col = 'red')
dev.off()
```
<br>  


#### Generate read count matrix to use for clustering
**Pseudo:**  
+ Take average of replicate values for each sample
+ Plot distributions of read counts before and after averaging reps
+ **Plot PCA after averaging reps** (TO DO)
+ Convert replicate averages to log2
+ Give each peak a unique ID
+ Export avg rpkm, log2rpkm of avgs 



#### **Deprecated** Generate read count matrix to use for clustering 
**Pseudo:**
+ Plot distributions of read counts in the merged peak set for all samples before averaging replicates
+ Take average of replicate values for each sample
+ Plot distributions of read counts after averaging reps
+ Convert replicate averages to log2
+ Give each peak a unique ID
+ Export avg rpkm, log2rpkm of avgs, 
```{r count_matrix}
##----plot read count distributions -------------------------------------------------------------

# import peak file
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/"
ttreg <- read.csv(file = paste(path,'a5-ttreg-consensus.rpkm.csv',sep=""), header =T)

# compare read count distributions of raw data
pdf(paste0(path,"a5-ttreg-merged-counts-rpkm.pdf"), width=8, height=6, pagecentre = T)
par(oma = c(4,1,0.5,1))
boxplot(log(ttreg[4:19],2), las = 2, cex.axis = 0.7, 
        ylab='log2 rpkm', main='merged atac peaks, n=37756')
dev.off()

# try rotated view
boxplot(log(ttreg[4:19],2), las = 2, cex.axis = 0.5, horizontal =T)

##----avg replicates and log2 convert ----------------------------------------------------------------
# take replicate averages and plot
library(dplyr)

# compute replicate averages 
ttregavg <- ttreg %>% # ttreg averages
  transmute(mean1 = rowMeans(.[4:5]),
            mean2 = rowMeans(.[6:7]),
            mean3 = rowMeans(.[8:9]),
            mean4 = rowMeans(.[10:11]),
            mean5 = rowMeans(.[12:13]),
            mean6 = rowMeans(.[14:15]),
            mean7 = rowMeans(.[16:17]),
            mean8 = rowMeans(.[18:19]))

# generate new column names
samples <- names(ttreg[-c(1:3)])
samples <- as.character(strsplit(samples, split = '_rep1.2'))
samples <- as.character(strsplit(samples, split='_rep[1,2,3]'))
samples <- unique(samples)

# rename columns
names(ttregavg) <- samples

# paste chr coordinates back on and add peak names and strand
peakName <- paste('peak_', 1:nrow(ttregavg),sep="")
strand <- '+'
ttregavg <- cbind(ttreg[1:3],PeakID = peakName, Strand = strand, ttregavg)

# boxplot average read count distributions
pdf(paste0(path,"a5-ttreg-rep-avg-counts-rpkm.pdf"), width=8, height=6, pagecentre = T)
par(oma = c(4,1,0.5,1))
boxplot(log(ttregavg[6:13],2), las = 2, cex.axis = 0.7, 
        ylab='rep average log2rpkm', main='merged atac peaks, n=37756, replicate averages')
dev.off()

# export df to file
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/"
# as .csv
write.csv(ttregavg, file=paste(path,'a5-ttreg-rep-avg-counts-rpkm.csv',sep=""), row.names=F) # as csv
# as .bed
write.table(ttregavg[1:5], file=paste(path,'a5-ttreg-rep-avg.bed',sep=""),row.names=F, 
            sep='\t', col.names = c('#Chr','Start','End','PeakID','Strand'))

# log2 convert rpkm and export to file
ttregavg[6:13] <- as.data.frame(lapply(ttregavg[6:13], log,2)) #ttreg[l]og2
write.csv(ttregavg, file=paste(path,'a5-ttreg-rep-avg-counts-log2rpkm.csv',sep=""), row.names=F) # as csv
```
<br>  


#### Annotate peaks with closest gene and functional element, use as rownames dim in read count matrix
+ on Orchestra, use HOMER `annotatepeaks.pl`
<br>
```{r annotate_merged_matrix, engine = 'bash'}
# load module
module load seq/homer/4.6

# annotate
annotatePeaks.pl a5-ttreg-rep-avg.bed mm9 > a5-ttreg-rep-avg.annotated.bed
```


#### Take samples of peaks, compare sample distributions to population
**Pseudo:**
+ Use peakID as rownames (would like to use GeneName or Functional Element but rownames have to be unique) 
+ Take random samples of different sizes 
+ Plot distributions of read counts in samples vs distribution in population    

**TO DO: figure out different scales on facets, and how to boxplot with var.width =T**
```{r sample_clustering}
# set path and filename
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/"
filename <- 'a5-ttreg-rep-avg-annotated.bed.txt'
# read in
ttreg <- read.delim(paste0(path,filename), header=T, check.names=T, sep='\t')

##---- add Peak IDs as rownames --------------------------------------------------------------------------
# check length of annotation want to use as rowID
length(ttreg$PeakID)

# combine count df and annotation df
ttregavg <- ttregavg[-c(1:3)]

ttregm <-  left_join(ttregavg, ttreg, by = 'PeakID') #[tt]reg[m]atrix
# combine gene name and rpm values
ttregm <- cbind(PeakID = ttregm$PeakID, ttregm[grep(colnames(ttregm), pattern = '+_+')])
# convert PeakIDs to row names
rownames(ttregm) <- ttregm$PeakID
# check
head(attributes(ttregm)[[1]])
# convert to matrix
ttregm <- as.matrix(ttregm[-1])
# save
write.csv(ttregm, file = paste0(path,'a5-ttreg-rep-avg-counts-log2rpkm-mat.csv'))
save(ttregm, file = paste0(path, 'a5-ttreg-rep-avg-counts-log2rpmk-mat.RDa'))

##---- sample peaks --------------------------------------------------------------------------------------
# read in matrix of peak scores
ttregm <- read.csv(file = paste0(path,'a5-ttreg-rep-avg-counts-log2rpkm-mat.csv'), row.names=1, header=T)
ttregm <- as.matrix(ttregm)

# vector of samples sizes
sizes <- c(1000,5000,10000,15000,20000) # vector of sample sizes
sl <- list() #[s]ample [l]ist

# generate sample matrices of different sizes, combine into list
for(i in 1:length(sizes)) {
  m <- matrix(nrow=1000,ncol=ncol(ttregm),data = 0)
  m <- ttregm[sample(nrow(ttregm), size=sizes[i],replace=F),]
  sl[[i]] <- m
}
# check matrices are correct sizes
sapply(sl, dim)

# name list elements
names(sl) <- paste('npeaks_',sizes,sep="") 
# save list to file
save(sl, file = paste0(path,'a5-ttreg-peak-sample-list.RDa'))

##---- plot -----------------------------------------------------------------------------------------------
# melt sample list to plot
library(reshape2)
library(ggplot2)
msl <- melt(sl)[-1] # [m]elted[s]ample[l]ist
names(msl) <- c('CellType','log2rpkm','PeakSample') # rename columns 
msl$PeakSample <- factor(msl$PeakSample, levels = unique(msl$PeakSample)) # convert peak sample ID to factor

# melt original complete matrix to plot
mpl <- melt(ttregm)[-1] # [m]elted[p]eak[l]ist
names(mpl) <- c('CellType','log2rpkm')
mpl$PeakSample <- 'allpeaks'

# combine the two lists
mtl <- rbind(msl,mpl) #[m]elted[t]otal[l]ist

# plot histograms
g <- ggplot(mtl, aes(x=log2rpkm))
g + geom_histogram(fill="white",color="black",binwidth=0.1) + 
  facet_grid(PeakSample ~.) +
  theme_bw()
# save
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/"
ggsave(file = paste(path,'a5-sample-pk-rpkm-hist.pdf'),width=8,height=7)

# plot densities
g <- ggplot(mtl, aes(x=log2rpkm))
g + geom_density() + 
  facet_grid(PeakSample ~.) +
  theme_bw()
# save
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/"
ggsave(file = paste(path,'a5-sample-pk-rpkm-dens.pdf'),width=8,height=7)
```
<br>    


#### Start by clustering random samples generated from set of all strong peaks
**Pseudo:**
+ hclust, different distance metrics     

**TO DO:**
+ clustering on entire sample, saving clustered mat, only plotting every other or fourth in heat
+ how to do this? cluster rows? then columns? how to save results?
```{r clust}
# import peak samples
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/"
filename <- 'a5-ttreg-peak-sample-list.RDa'
load(paste0(path,filename))

# look over
str(sl)
names(sl)
str(sl[[1]])
head(sl[[1]])
summary(sl[[3]])

## for clustering, keep each peak sample matrix in list and refer to by list index

##---- visualize on small sample -----------------------------------------------------------------------------
# get nice colors
library(RColorBrewer)

# display spectra
display.brewer.all()

# set colors
niceCols <- colorRampPalette(rev(brewer.pal(11, 'Spectral')))(50)

# test default heatmap
library(pheatmap)

# heatmap using default hclust on peaks(rows) and samples(columns)
pheatmap(sl[[3]], color = niceCols)
# visualize every 200th peak
pheatmap(sl[[3]][seq(from = 5000, to = nrow(sl[[3]]), by = 200),],color = niceCols)


## matplots on select peaks
npeaks10K <- sl[[3]]
str(npeaks10K)

# extract small subsets
set1 <- c('peak_20075','peak_5710','peak_25150','peak_24139')
set2 <- c('peak_22214','peak_20117','peak_34264')

# have to transpose mat for matplot
matplot(t(sl[[3]][set1,]),
        type='l',lwd=2,col='skyblue',lty=1,
        ylim=c(-10,10), xlab='Cell Type',ylab = 'Log2 rpkm')

# use lines() to superimpose values for set2
for (i in 1:length(set2)) {
  lines(sl[[3]][set2[i],], type = 'l', lwd=2, col='firebrick')
}


##----try hclust -------------------------------------------------------------------------------------------
# try different clustering methods with each distance function
# distance: 'euclidean','maximum','manhattan','canberra','binary','minkowski','correlation'
# methods: 'ward.D','ward.D2','single','complete','average','mcquitty','median','centroid'

pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'ward.D')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'ward.D2')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'single')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'average')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'mcquitty')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'median')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'centroid')
## results:
# distance: minkowski > manhattan > max >= euclidean > canberra >> correlation >>> binary
# method: ward > complete = average = single = mcquitty
# so far best: mink on both+ward.D2 (really, all method similar), max+ward.D2, euclidean + ward.D, euclid (avg=single=mcq=median)



# try clustering on row-scaled or column-scaled data
# distance: 'euclidean','maximum','manhattan','canberra','binary','minkowski','correlation'
# methods: 'ward.D','ward.D2','single','complete','average','mcquitty','median','centroid'
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski', 
         scale = 'column')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'ward.D', scale = 'column')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'ward.D2', scale = 'column')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'single', scale = 'column')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'average', scale = 'column')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'mcquitty', scale = 'column')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'minkowski',
         cluster_method = 'median', scale = 'column')
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowksi', clustering_distance_cols = 'minkowski',
         cluster_method = 'centroid', scale = 'column')
## results:
# distance: euclidean = max = manhat = canberra = mink = corr >> binary, all bad and VAT cluster far from muscle
# method: for all methods
# so far best: none ; row is bad, column-scaling is slightly better but not as good as no scale

## try different row-clust mixed with column by pearson? or euclidean?
pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'correlation')

pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'correlation', cluster_method = 'ward.D2')
## results:
# not that different from using same on rows and cols

##---- try kmeans function in pheatmap() --------------------------------------------------------------------

set.seed(123456)
# save results of pheatmap to object p
p <- pheatmap(sl[[3]], color= niceCols, clustering_distance_rows = 'minkowski', clustering_distance_cols = 'correlation', kmeans_k = 10)

# extract results of kmeans
k <- p$kmeans
# save
save(k, file = paste0(path,'a5-km.RDa'))

# extract kmeans clusters
c <- p$kmeans$cluster 
# convert cluster and peak IDs to df
cdf <- data.frame(peak = names(c), cluster = c, row.names= NULL) # [c]luster[d]ata[f]rame
# convert sample 3 (n = 10000 peaks) to dataframe
sl3df <- data.frame(peak = rownames(sl[[3]]), sl[[3]], row.names=NULL) #[s]ample[l]ist[3][d]ata[frame]
# add clusterID onto df
library(dplyr)
sl3dfc <- left_join(sl3df, cdf, by = 'peak') # sl3df[c]lusters

# check that join worked properly by looking up rpm values in original sample matrix
sl[[3]][rownames(sl[[3]])=='peak_5710',]
sl[[3]][rownames(sl[[3]])=='peak_23145',]
# save
save(sl3dfc, file=paste0(path,'a5-sl3dfc.RDa'))
# load
load(paste0(path,'a5-sl3dfc.RDa'))

# order df
sl3dfco <- sl3dfc[order(sl3dfc$cluster),] # sl3dfc[o]rdered

# save
save(sl3dfco, file = paste0(path,'a5-sl3dfco.RDa'))

# redraw heatmap based on kmeans
pdf(paste0(path,"a5-pheatmap-km-k10.pdf"), width=10, height=10)
png(paste0(path,"a5-pheatmap-km-k10.png"), width=500, height=500)
pheatmap(as.matrix(sl3dfco[2:9]), cluster_rows = F, color = niceCols)
# save 
dev.off()


## try matplotting some km clusters
# convert peakIDs into rownames
sl3dfc.rn <- data.frame(sl3dfc[-1], row.names=sl3dfc$peak)
# re-order columns to put all t-treg together
sl3dfc.rn <- sl3dfc.rn[c(9,1,2,3,8,4,5,6,7)]

# extract cluster 1
c1 <- as.matrix(sl3dfc.rn[2:9][sl3dfc$cluster ==1,], )
c10 <- as.matrix(sl3dfc.rn[2:9][sl3dfc$cluster ==10,], )
# plot clusters
matplot(t(c10), type='l',xlab='cell type',ylab='log2 rpkm', xlim=c(1,8))

# extract each cluster and combine into a list
cl <- list() # [c]luster [l]ist
for (i in 1:10) {
  cl[[i]] <- as.matrix(sl3dfc.rn[2:9][sl3dfc$cluster == i,],)
}

# loop over to plot each cluster
pdf(paste0(path,"a5-km-k10-matplots.pdf"), width=14, height=10)

par(mfrow = c(2,5))
for (i in 1:10) {
  matplot(t(cl[[i]]),
          type = 'l', col = niceCols[i],
          xlab = 'Tissue Treg', ylab= 'log2 rpkm',
             xlim=c(1,8),ylim=c(-2,12))
}
dev.off()

#### ------ in progress
## have to combine clusters with peak coords
# set path and filename
path <- "/Volumes/JD_WD2/JRD_CBDMLab/1_JRD_CBDM_PROJECTS/ProjectFolders/T-TregEpigenome/Data/1_Experimental/ATAC/Analyses/A5-all-ttreg/"
filename <- 'a5-ttreg-rep-avg-annotated.bed.txt'
# read in peak file with peakID
ttreg <- read.delim(paste0(path,filename), header=T, check.names=T, sep='\t')
# need to check that these match clustering

....

# convert back to matrix with rownames == clusterID 
# cant with this code bc rownames not unique!!
# how to get around?
test <- data.matrix(sl3dfco[2:9, rownames.force = sl3dfco$cluster])


##---- try kmeans from algorithm ------------------------------------------------------------------------
k <- 10
cl <- kmeans(as.matrix(sl3dfc.rn[2:9]),k)
niceCols <- brewer.pal(k, "Spectral")
# can see that some clusters not nicely separated betwen vat and spleen
plot(as.matrix(sl3dfc.rn[,4]),as.matrix(sl3dfc.rn[,8]), col=niceCols[cl$cluster]) 
points(cl$centers, col = niceCols[1:k], pch = 8, cex=2)


##---- try kmediods from algorithm ------------------------------------------------------------------------
library(cluster)
set.seed(112358)
k <- 10
cl <- pam(sl[[3]],k)
# separation not much better than kmeans
plot((sl[[3]][,3]),sl[[3]][,8], col=niceCols[cl$cluster], cex=0.4) 

plot(cl) # shows boundary and silhouette plots


##---- try APclustering  ----------------------------------------------------------------------------------

```
<br>  

#### Clustering on all peaks after removing outliers
**Pseudo:**
+ define outliers (1.5*IQR?) 
+ go back to full peak list
+ loop definition over each column, create logical matrix with dim == dim(sl[[n]])
+ subset on logical == F
+ sample and combine samples into list as above
+ call new set 'slno' [s]ubset[l]ist[n]o[outliers]
<br>  


#### Clustering on only the variable peaks OR separately on similar and variable
**Pseudo:**
+ decide between using variance or FC >= n in >=1 pair-wise   
+ could do FC >=4 --> hc ; FC <4 --> hc
+ what if row-scale after finding good clusters


#### Finding best heatmap colors for clusters
```{r colors}
eqSpect <- colorRampPalette(
               c("#f2003c", "#F66900", "#F19100", "#F1B100",
                 "#EFD300", "#f0ea00", "#CBDB00", "#9DD501",
                 "#5ED108", "#00AF63", "#00A78E", "#00a3ac",
                 "#0093af", "#0082b2", "#006ebf", "#4F37C2",
                 "#8000D3", "#A001BF", "#BA00A4", "#D0007F"),
                 space="rgb",
                 interpolate="spline")
n <- 10

niceCols <- eqSpect(n)

# or 
niceCols <- colorRampPalette(rev(brewer.pal(10, 'Spectral')))(50)
niceCols <- rev(brewer.pal(10, 'Spectral'))
```
<br>  




#### Deprecated
```{r deprecated}
## ---- deprecated ------------------------------------------------------------

#niceCols <- rev(brewer.pal(8, 'Spectral'))
#niceCols <- colorRampPalette(rev(brewer.pal(n=9,name ='RdYlBu')))(100)

## Different distance functions using pheatmap
# df <- function(x) dist(x, method="euclidean") 
# heatmap(dat[seq(1,nrow(dat),by =4), ], labRow="", labCol="", distfun = df)

# cluster and plot every other peak
# pheatmap(sl[[3]][seq(from = 1, to = nrow(sl[[3]]), by = 2),], color=niceCols) 

# extract small subsets
set1 <- c('TPX2','CCNA2','AURKA','CEP55','CCNB1')
set2 <- c('MAB21L3','CCNE1','TCF19///TF19','ZBTB14')
set2 <- c('MAB21L3','CCNE1','ZBTB14')


## ----- working
# make distance matrix
distDat <- dist(sl[[1]])

# perform default hc = complete linkage hc
hc <- hclust(distDat)
plot(hc)

# as an alternative, try Wards-linkage clustering
hc.ward <- hclust(distDat, method = 'ward.D', members=NULL)
plot(hc.ward)

# tried sub-ordering w/in single cluster, looks weird
sl3dfco2 <- sl3dfc[order(sl3dfc$cluster,sl3dfc$Spl_wMus_Treg,sl3dfc$Spl_wVAT_Treg,
                         sl3dfc$Spl_wCln_Nrp1n_Treg, sl3dfc$Spl_wCln_Nrp1p_Treg,
                         sl3dfc$Vat_Treg, sl3dfc$Mus_Treg, sl3dfc$Cln_Nrp1n_Treg,
                         sl3dfc$Cln_Nrp1p_Treg),]
```





